{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU3_aRitCbl",
        "outputId": "8cb06144-cbff-4651-8ae1-433603233633"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSfTikpK0Bu5",
        "outputId": "babf4b5d-6ac4-4500-f5c9-f4bfe5372ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.21.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.0 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RfmpkMqT1daE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "OPENAI_API_KEY = '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'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XajkO-wEtnXp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key =  OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srz45bfF0DJ1"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfWpy7_QVC8t"
      },
      "outputs": [],
      "source": [
        "doc = fitz.open('/content/Attention.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ni19lQsoVJKP"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "\n",
        "text = \"\"\"\n",
        "Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by means of minimizing errors between actual and expected results. Further, gradient descent is also used to train Neural Networks.\n",
        "\n",
        "In mathematical terminology, Optimization algorithm refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. Similarly, in machine learning, optimization is the task of minimizing the cost function parameterized by the model's parameters. The main objective of gradient descent is to minimize the convex function using iteration of parameter updates. Once these machine learning models are optimized, these models can be used as powerful tools for Artificial Intelligence and various computer science applications.\n",
        "\"\"\"\n",
        "\n",
        "prompt= text + \"\\n Tl;dr:\"\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=120,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUjynEGNXcbT",
        "outputId": "50c5321b-fe27-4a1c-a960-e1dd44edf1ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Gradient Descent is an optimization algorithm used to train machine learning models by minimizing errors between actual and expected results. It is also used to train Neural Networks and minimize the cost function parameterized by the model's parameters.\n"
          ]
        }
      ],
      "source": [
        "summary_text=' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "lrN3AgYzZHzd",
        "outputId": "2212719c-c51d-412b-89e2-9da549b6b1d4"
      },
      "outputs": [
        {
          "ename": "AuthenticationError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4bb675348b21>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msummary_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n Tl;dr:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-nE3uT***************************************BZdx. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ],
      "source": [
        "prompt= summary_text + \"\\n Tl;dr:\"\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np0zcc0IajDP",
        "outputId": "080ce32a-c04b-40f9-fd65-2a2b806520b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" This paper presents a new architecture, the Transformer, which relies on attention mechanisms instead of recurrence and is more parallelizable. Experiments on two machine translation tasks show that the model achieves superior results, improving over the existing best results by 2 BLEU points. It contains an encoder and decoder stack with multi-head self-attention mechanisms and position-wise fully connected feed-forward networks. Attention heads in the encoder self-attention layer have learned to perform different tasks related to the structure of the sentence.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1670219783,\n",
            "  \"id\": \"cmpl-6Jykhkghj0VSQG0tokyjPRpg6w9OA\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 108,\n",
            "    \"prompt_tokens\": 1262,\n",
            "    \"total_tokens\": 1370\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4WmT6W5ePIk"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= \"Summarize this for a second-grade student: \" +text\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUYSaUW6f8-H",
        "outputId": "e5f381a3-b882-4101-e96a-e58f2ce35419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Scientists have created a new type of network architecture called the Transformer that uses attention mechanisms instead of recurrence and convolutions to help machines understand and translate language. Experiments have shown that this model is faster and more accurate than other models that have been used before. It has helped machines to do things like translate English into German or French and understand how words fit together in a sentence. A Transformer is a type of model used for language modeling and machine translation. It uses attention mechanisms to draw global dependencies between input and output, which allows for more parallelization and faster training times. For a second-grade student:\n",
            "The Transformer is a model that helps machines understand and generate language. It has two parts - an encoder which takes in the language and a decoder which produces the output language. It also uses something called attention, which is like a way for the machine to remember important words from the input language and use them in the output language. \n",
            "Multi-Head Attention is a type of attention used in computer programming. It consists of several layers that run in parallel, each layer taking the queries, keys, and values and projecting them into different dimensions. The output is then combined and projected to create the final values. Scaled Dot-Product Attention is a type of Multi-Head Attention which scales the dot products by 1 divided by the square root of the dimension of the keys. This helps make sure that the softmax function works correctly. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the\n",
            "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
            "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
            "learned and fixed [18].\n",
            "\n",
            "Multi-head attention is a way for a computer model to look at different parts of a sentence or other text at the same time. It uses 8 different \" Self-attention is a way of connecting each part of a sequence to every other part. It can be used to help machines understand longer sequences, like sentences. Self-attention layers take less time than recurrent layers and have shorter paths between long-range dependencies. A second-grade student might understand this as: Scientists are trying to create a better way for computers to understand language. They are using something called \"self-attention\" which helps computers understand the words and their meanings in sentences. They use layers of different parts in their models to help with this. This process is complex and takes time, but it may help computers better understand language. A second-grade student can understand that the Transformer is a machine learning model that can achieve better scores than other models on English-to-German and English-to-French tests, while using less training cost. It achieved the highest BLEU score of 28.4 on English-to-German translation and 41.0 on English-to-French translation. \n",
            "The Transformer is a type of architecture that can help with translating and understanding language. It can be changed in different ways to make it work better. When the size of the model is bigger and dropout is used, it works better. The Transformer also works well when doing English constituency parsing which is a type of language understanding. A computer model called the Transformer was trained on 40,000 sentences to learn how to translate between languages. It was able to outperform other models and is now used to help computers understand human language better. Four people (Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio) studied how computers can understand things that are written or said. They used something called a recurrent neural network to help them. Four people, Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit, wrote a paper about a special type of computer model. Romain Paulus, Caiming Xiong, and Richard Socher wrote a paper about a deep learning model that helps computers summarize information. Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein wrote a paper about teaching computers to understand tree diagrams. Oﬁr Press and Lior Wolf wrote a paper about making computers better at language. Rico Senn \n",
            "Many governments in America have passed new laws since 2009 which make it harder to register or vote. We need to make sure that the law is applied fairly and justly, which is something we are missing. The law can never be perfect, but it should be applied in a just way - this is something we are missing.\n"
          ]
        }
      ],
      "source": [
        "summary_text= ' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nScXhI01gDiK",
        "outputId": "d64e1c82-ecea-4bb8-c42e-72e7f9f45afe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Scientists have been looking for ways to make computers understand language better. They have created a model called the Transformer that uses attention mechanisms and is able to translate quickly and accurately. This model has been tested on two machine translation tasks and was found to be more accurate and faster than existing methods. It also works well for English constituency parsing.\n"
          ]
        }
      ],
      "source": [
        "prompt= \"Summarize this for a second-grade student: \"+summary_text\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3hZO60FgU_3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here will be covered : \n",
    "- Sentence Embedding\n",
    "- Sentence Similarity\n",
    "- Semantic Search\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- https://www.sbert.net/index.html\n",
    "- https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers\n",
    "\n",
    "# sentence-transformers (BERT) is a library for computing sentence embeddings (sentence vectors) in Python and PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# all-MiniLM-L6-v2 is a transformer model trained on a large corpus of English sentences and their paraphrases. It is a smaller version of the MiniLM model (6 layers instead of 12 layers). It was trained on the AllNLI and STS benchmark datasets. And it is the best performing model on the STS benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['This framework generates embeddings for each input sentence','Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.\n",
    "\"\"\"\n",
    "text3 = \"\"\"\n",
    "Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.\n",
    "\"\"\"\n",
    "\n",
    "text4 = \"\"\"\n",
    "Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice.\n",
    "\"\"\"\n",
    "\n",
    "text5 = \"\"\"\n",
    "Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by means of minimizing errors between actual and expected results. Further, gradient descent is also used to train Neural Networks.\n",
    "\n",
    "In mathematical terminology, Optimization algorithm refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. Similarly, in machine learning, optimization is the task of minimizing the cost function parameterized by the model's parameters. The main objective of gradient descent is to minimize the convex function using iteration of parameter updates. Once these machine learning models are optimized, these models can be used as powerful tools for Artificial Intelligence and various computer science applications.\n",
    "\"\"\"\n",
    "\n",
    "# This is the summerized version of text5\n",
    "text6 = \"\"\"\n",
    " Gradient Descent is an optimization algorithm used to train machine learning models by minimizing errors between actual and expected results. It is also used to train Neural Networks and minimize the cost function parameterized by the model's parameters.\n",
    "\"\"\"\n",
    "\n",
    "text7 = \"\"\"\n",
    "The Fear Nothing Blood Test is able to give you an accurate understanding of your health by checking several key health indicators. The standard Fear Nothing Blood Test can tell you about your: Vitamin levels. Hormone levels. Liver health.\n",
    "\"\"\"\n",
    "\n",
    "text8 = \"\"\"\n",
    "This subject only gives a brief description about different types of materials used in building construction for members like foundation, masonry, arches, lintels, balcony, roof, floor, doors, windows, stairs, plastering, painting and other general topics. Properties of various construction materials, their uses and different applications are discussed in this subject. \n",
    "\"\"\"\n",
    "\n",
    "text9=\"\"\"\n",
    "Initial setting time for ideal cement mix is around 30 minutes for almost all kind of cements. For masonry cement it can be 90 minutes. Final setting time of ideal cement mix should be 10 hours at max. For masonry cement it shouldnâ€™t exceed 24 hours.\n",
    "\"\"\"\n",
    "emd1 = model.encode(text1)\n",
    "emd2 = model.encode(text2)\n",
    "# emd2 = model.encode(\"Tom deserves unbiased judgement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity = util.cos_sim(emd1, emd2)\n",
    "cos_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute Cosine Similarity Between all pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'A man is eating food.',\n",
    "    'A man is eating a piece of bread.',\n",
    "    'A man is riding a horse.',\n",
    "    'The girl is carrying a baby.',\n",
    "    'A woman is playing violin.',\n",
    "    'two men pushed carts through the woods.',\n",
    "    'A man is riding a white horse on an enclosed ground.',\n",
    "    'A monkey is playing drums.',\n",
    "    'A cheetah is running behind its prey.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enode sentences to get their embeddings\n",
    "embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cosine_scores = util.cos_sin(embeddings, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cosine_scores)-1):\n",
    "    for j in range(i+1, len(cosine_scores)):\n",
    "        all_sentence_combinations.append([cosine_scores[i][j], i, j])\n",
    "\n",
    "# Sort in decreasing order of the cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# x[0] is the criteria for sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the pairs according to their cosine similarity score\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t\\t {} \\t\\t {:.4f}\".format(sentences[i], sentences[j], cosine_scores[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('clips/mfaq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"<Q>How many models can I host on Hugging Face?</Q>\"\n",
    "answer_1 = \"<A>All plans come with unlimited private models and datasets.\"\n",
    "answer_2 = \"<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.\"\n",
    "answer_3 = \"<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(question)\n",
    "corpus_embeddings = model.encode([answer_1, answer_2, answer_3])\n",
    "\n",
    "results = util.semantic_search(query_embedding, corpus_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering\n",
    "\n",
    "Basically for question answering we have to perfrom also the previous tasks\n",
    "Sequence is : \n",
    "1. First find the semantic similarity by ranking existing document\n",
    "2. Then Semantic search\n",
    "3. Find the top document -> Finnaly the QA step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"How many models can I host on HuggingFace?\"\n",
    "context = \"All plans come with unlimited private models and datasets.\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'Horse is eating grass.',\n",
    "          'A man is eating pasta.',\n",
    "          'A Woman is eating Biryani.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'The baby is carried by the woman',\n",
    "          'A man is riding a horse.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.',\n",
    "          'A cheetah is running behind its prey.',\n",
    "          'A cheetah chases prey on across a field.',\n",
    "          'The cheetah is chasing a man who is riding the horse.',\n",
    "          'man and women with their baby are watching cheetah in zoo'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus)\n",
    "\n",
    "# Normalize the embeddings to unit length\n",
    "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert\n",
    "\n",
    "clustering_model = KMeans(n_clusters=4)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "print(cluster_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "clustered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = {}\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "clustered_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

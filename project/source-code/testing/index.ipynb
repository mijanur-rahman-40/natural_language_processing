{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mijanur/Desktop/AI/DL NLP/natural_language_processing/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 'I like to eat apples.'\n",
    "t2 = 'I like to play.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert-base-uncased\n",
    "class Bert(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert, self).__init__()\n",
    "        self.bert = models.Transformer('sentence-transformers/all-MiniLM-L6-v2', max_seq_length=128)\n",
    "        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n",
    "        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = self.sts_bert(input_data)['sentence_embedding']\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = 'bert-sts.pt'\n",
    "PATH =\"bert-sts-org.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Replace 'bert-base-uncased' with the correct pre-trained model identifier\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', hidden_size=768)\n",
    "\n",
    "# Initialize your BERT model with the specified configuration\n",
    "model = BertModel(config)\n",
    "\n",
    "# Load the pre-trained weights from checkpoint\n",
    "model.load_state_dict(torch.load('bert-sts-org.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert()\n",
    "model.load_state_dict(torch.load(PATH), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def predict_similarity(sentence_pair):\n",
    "  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n",
    "  test_input['input_ids'] = test_input['input_ids']\n",
    "  test_input['attention_mask'] = test_input['attention_mask']\n",
    "  del test_input['token_type_ids']\n",
    "  output = model(test_input)\n",
    "  similarity = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n",
    "\n",
    "  return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4813702702522278"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_similarity([t1,t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Globalization is a multifaceted process involving the increasing interconnectedness and interdependence of people, economies, cultures, and societies across the world. It is facilitated by advancements in communication, transportation, and technology, allowing for the rapid exchange of goods, services, information, and ideas across borders.\\n\\nGlobalization has various dimensions:\\n\\n1. Economic Globalization: Involves the integration of national economies into the global economy through trade, investment, and the flow of capital.\\n\\n2. Cultural Globalization: Refers to the spread of cultural products, values, and practices around the world, leading to cultural homogenization and hybridization.\\n\\n3. Technological Globalization: Involves the diffusion and adoption of technologies worldwide, shaping communication and information-sharing.\\n\\n4. Political Globalization: Encompasses the growing influence of international organizations, treaties, and agreements in shaping global governance.\\n\\nGlobalization has both positive and negative consequences. On one hand, it has facilitated economic growth, cultural exchange, and technological advancements. On the other hand, it has also raised concerns about inequality, exploitation, cultural erosion, and environmental impacts.\\n\\nUnderstanding globalization is crucial in analyzing global issues, international cooperation, and the challenges of an interconnected world.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

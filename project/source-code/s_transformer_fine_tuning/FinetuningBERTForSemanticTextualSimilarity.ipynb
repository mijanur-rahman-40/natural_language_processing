{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Cu87RMWw-P"
      },
      "source": [
        "### 1. Install and import the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Px8aik4VaOY",
        "outputId": "174afa48-bafe-404f-f702-caf4661fb44b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=d65b8b0531174da075b638847e06264f9449fa92426c78097e34f966ed1d9af7\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, dill, multiprocess, huggingface-hub, transformers, datasets, sentence-transformers\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.4 multiprocess-0.70.14 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentence-transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUsTXFi1bNRI"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMdAdDQbzWmC"
      },
      "source": [
        "### 2. Use Google Colab's GPU for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB7TNNSrziMu",
        "outputId": "bc97d79b-b61c-441e-d4d6-7c5dea18f326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ1Eel-3W-5b"
      },
      "source": [
        "### **3.** Load and preview the Semantic Textual Similarity Benchmark (STSB) dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "ffc89922029e4477ac375e1fd03c80c2",
            "2021df6eafec42569f5514863cfbfe02",
            "e807fb79b6ef42ffa6dfa71459194d1a",
            "98071870cad74395ab14e13d6ded0768",
            "ed9115ea224443f3b9780aa1195fcf2b",
            "09a66f4e0d05466091b55922da51f655",
            "367159fb757f4a6cbaccadee2b0725fe",
            "d999fd242719460aa8c5040e217099e5",
            "b19e182902114902a934a8623a541849",
            "63558b296da94379a0aaa318a2e03160",
            "e4be22f8c98740a4abefe346610a9dcd"
          ]
        },
        "id": "mgwlDDjtWM71",
        "outputId": "e3b63f16-a98e-4545-cc51-48d77a8fc9b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffc89922029e4477ac375e1fd03c80c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.43k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2021df6eafec42569f5514863cfbfe02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/19.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e807fb79b6ef42ffa6dfa71459194d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.98k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset stsb_multi_mt/en to /root/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98071870cad74395ab14e13d6ded0768",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed9115ea224443f3b9780aa1195fcf2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/229k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09a66f4e0d05466091b55922da51f655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367159fb757f4a6cbaccadee2b0725fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/52.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d999fd242719460aa8c5040e217099e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b19e182902114902a934a8623a541849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63558b296da94379a0aaa318a2e03160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating dev split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset stsb_multi_mt downloaded and prepared to /root/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4be22f8c98740a4abefe346610a9dcd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load the English version of the STSB dataset\n",
        "dataset = load_dataset(\"stsb_multi_mt\", \"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtUWgi0h_DjR",
        "outputId": "6ff08b31-bb6b-4b77-d737-cb6b19b1d2f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
            "        num_rows: 5749\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
            "        num_rows: 1379\n",
            "    })\n",
            "    dev: Dataset({\n",
            "        features: ['sentence1', 'sentence2', 'similarity_score'],\n",
            "        num_rows: 1500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEHZl4WeWv6r",
        "outputId": "bfe16cd1-d0e0-4f02-ecd5-4816fe143333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A sample from the STSB dataset's training split:\n",
            "{'sentence1': 'A man is slicing potatoes.', 'sentence2': 'A woman is peeling potato.', 'similarity_score': 2.200000047683716}\n"
          ]
        }
      ],
      "source": [
        "print(\"A sample from the STSB dataset's training split:\")\n",
        "print(dataset['train'][98])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjMKsIuxYv6D"
      },
      "source": [
        "### **4.** Define the dataset loader class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "9ebce8952f22439f889237e847e99e94",
            "09e78f86b8f641dab7c829237c2ba06f",
            "e8c348cfbf134155864bdb1d14385588"
          ]
        },
        "id": "f2Hc2uwabgJa",
        "outputId": "2ba5e884-65d7-4121-cda9-c1c350f17a2e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ebce8952f22439f889237e847e99e94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09e78f86b8f641dab7c829237c2ba06f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8c348cfbf134155864bdb1d14385588",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Instantiate the BERT tokenizer\n",
        "# You can use larger variants of the model, here we're using the base model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEI1p5-SaM8t"
      },
      "outputs": [],
      "source": [
        "class STSBDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "\n",
        "        # Normalize the similarity scores in the dataset\n",
        "        similarity_scores = [i['similarity_score'] for i in dataset]\n",
        "        self.normalized_similarity_scores = [i/5.0 for i in similarity_scores]\n",
        "        self.first_sentences = [i['sentence1'] for i in dataset]\n",
        "        self.second_sentences = [i['sentence2'] for i in dataset]\n",
        "        self.concatenated_sentences = [[str(x), str(y)] for x,y in zip(self.first_sentences, self.second_sentences)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.concatenated_sentences)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return torch.tensor(self.normalized_similarity_scores[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "\n",
        "def collate_fn(texts):\n",
        "    input_ids = texts['input_ids']\n",
        "    attention_masks = texts['attention_mask']\n",
        "    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n",
        "                for input_id, attention_mask in zip(input_ids, attention_masks)]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUICztAD0clm",
        "outputId": "abe1884d-254a-433d-c939-c5516c497e72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A plane is taking off.',\n",
              " 'A man is playing a large flute.',\n",
              " 'A man is spreading shreded cheese on a pizza.',\n",
              " 'Three men are playing chess.',\n",
              " 'A man is playing the cello.']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train']['sentence1'][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ICUkr20JbP"
      },
      "source": [
        "### 5. Define the model class based on BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgTYEHC8b7kb"
      },
      "outputs": [],
      "source": [
        "class BertForSTS(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForSTS, self).__init__()\n",
        "        self.bert = models.Transformer('bert-base-uncased', max_seq_length=128)\n",
        "        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n",
        "        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = self.sts_bert(input_data)['sentence_embedding']\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMNCebmb4Hlt",
        "outputId": "4ff6b0f9-87e0-4127-d330-a5102eb4823d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSTS(\n",
              "  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  (sts_bert): SentenceTransformer(\n",
              "    (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the model and move it to GPU\n",
        "model = BertForSTS()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXqIA_D_2nYC"
      },
      "source": [
        "### 6. Define the Cosine Similarity loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty7Q630Ob96f"
      },
      "outputs": [],
      "source": [
        "class CosineSimilarityLoss(torch.nn.Module):\n",
        "    def __init__(self,  loss_fn=torch.nn.MSELoss(), transform_fn=torch.nn.Identity()):\n",
        "        super(CosineSimilarityLoss, self).__init__()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.transform_fn = transform_fn\n",
        "        self.cos_similarity = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, inputs, labels):\n",
        "        emb_1 = torch.stack([inp[0] for inp in inputs])\n",
        "        emb_2 = torch.stack([inp[1] for inp in inputs])\n",
        "        outputs = self.transform_fn(self.cos_similarity(emb_1, emb_2))\n",
        "        return self.loss_fn(outputs, labels.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CUR9yuIzZj2",
        "outputId": "3b72f9ec-c867-49a8-821a-c94b55ed1461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence1', 'sentence2', 'similarity_score'],\n",
              "    num_rows: 5749\n",
              "})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B688H4qY26ZG"
      },
      "source": [
        "### 7. Prepare the training and validation data split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrQvEJgC4VeB",
        "outputId": "729c000f-cd3c-4fd4-cc85-2e9663cb5f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5,749 training samples\n",
            "1,500 validation samples\n"
          ]
        }
      ],
      "source": [
        "train_ds = STSBDataset(dataset['train'])\n",
        "val_ds = STSBDataset(dataset['dev'])\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "train_size = len(train_ds)\n",
        "val_size = len(val_ds)\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUPorlzExygm",
        "outputId": "addfe4e1-1b89-4e5a-b33d-bbe8328e9f7d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_ds,  # The training samples.\n",
        "            num_workers = 4,\n",
        "            batch_size = batch_size, # Use this batch size.\n",
        "            shuffle=True # Select samples randomly for each batch\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_ds,\n",
        "            num_workers = 4,\n",
        "            batch_size = batch_size # Use the same batch size\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5avkJtGn2-al"
      },
      "source": [
        "### 8. Define the Optimizer and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lB_HcVbl3EZw"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVT3cA_-3NPP"
      },
      "outputs": [],
      "source": [
        "epochs = 3\n",
        "# epochs = 8\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyIxF_7J3ep5"
      },
      "source": [
        "### 9. Define a helper function for formatting the elapsed training time as `hh:mm:ss`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH7_0ASp3oDW"
      },
      "outputs": [],
      "source": [
        "# Takes a time in seconds and returns a string hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJFhpUJp92Qe"
      },
      "source": [
        "### 10. Define the training function, and start the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdeUXU915NE5"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  seed_val = 42\n",
        "\n",
        "  criterion = CosineSimilarityLoss()\n",
        "  criterion = criterion.to(device)\n",
        "\n",
        "  random.seed(seed_val)\n",
        "  torch.manual_seed(seed_val)\n",
        "\n",
        "  # We'll store a number of quantities such as training and validation loss,\n",
        "  # validation accuracy, and timings.\n",
        "  training_stats = []\n",
        "  total_t0 = time.time()\n",
        "\n",
        "  for epoch_i in range(0, epochs):\n",
        "\n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      total_train_loss = 0\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "          train_data['input_ids'] = train_data['input_ids'].to(device)\n",
        "          train_data['attention_mask'] = train_data['attention_mask'].to(device)\n",
        "\n",
        "          train_data = collate_fn(train_data)\n",
        "          model.zero_grad()\n",
        "\n",
        "          output = [model(feature) for feature in train_data]\n",
        "\n",
        "          loss = criterion(output, train_label.to(device))\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "      print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "      # ========================================\n",
        "      #               Validation\n",
        "      # ========================================\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Running Validation...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      total_eval_accuracy = 0\n",
        "      total_eval_loss = 0\n",
        "      nb_eval_steps = 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for val_data, val_label in tqdm(validation_dataloader):\n",
        "\n",
        "          val_data['input_ids'] = val_data['input_ids'].to(device)\n",
        "          val_data['attention_mask'] = val_data['attention_mask'].to(device)\n",
        "\n",
        "          val_data = collate_fn(val_data)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              output = [model(feature) for feature in val_data]\n",
        "\n",
        "          loss = criterion(output, val_label.to(device))\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"  Validation Loss: {0:.5f}\".format(avg_val_loss))\n",
        "      print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Training Loss': avg_train_loss,\n",
        "              'Valid. Loss': avg_val_loss,\n",
        "              'Training Time': training_time,\n",
        "              'Validation Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Training complete!\")\n",
        "\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  return model, training_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calulate training loss\n",
        "model, training_stats = train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "CoWW_TnZgSRf",
        "outputId": "0b8f3ac7-43eb-486f-9640-bce464ddbd46"
      },
      "outputs": [],
      "source": [
        "# Launch the training\n",
        "model, training_stats = train()\n",
        "\n",
        "[{'epoch': 0,\n",
        "  'train_loss': 0.0828960295766592,\n",
        "  'valid_loss': 0.04938269406557083},\n",
        " {'epoch': 1,\n",
        "  'train_loss': 0.04546177061274648,\n",
        "  'valid_loss': 0.04938269406557083},\n",
        " {'epoch': 2,\n",
        "  'train_loss': 0.0731061939150095,\n",
        "  'valid_loss': 0.04938269406557083}]\n",
        "\n",
        "# convert train_loss and valid_loss to array\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "for i in range(len(training_stats)):\n",
        "  train_loss.append(training_stats[i]['Training Loss'])\n",
        "  valid_loss.append(training_stats[i]['Valid. Loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate train accuracy\n",
        "# it will be performerd after the model is saved and loaded\n",
        "train_accuracy = 0\n",
        "for train_data, train_label in tqdm(train_dataloader):\n",
        "    \n",
        "        train_data['input_ids'] = train_data['input_ids'].to(device)\n",
        "        train_data['attention_mask'] = train_data['attention_mask'].to(device)\n",
        "    \n",
        "        train_data = collate_fn(train_data)\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            output = [model(feature) for feature in train_data]\n",
        "    \n",
        "        train_accuracy += np.sum(np.round(output) == train_label.to(device).cpu().numpy())\n",
        "\n",
        "print(\"Train Accuracy: {}\".format(train_accuracy/len(train_ds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_similarity(sentence_pair):\n",
        "  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n",
        "  test_input['input_ids'] = test_input['input_ids']\n",
        "  test_input['attention_mask'] = test_input['attention_mask']\n",
        "  del test_input['token_type_ids']\n",
        "  output = model(test_input)\n",
        "  similarity = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n",
        "\n",
        "  return similarity\n",
        "\n",
        "train_examples = [\n",
        "    {\n",
        "        \"sentence1\" :\"To simulate the behaviour of portions of the desired software product.\",   \n",
        "        \"sentence2\" :\"I like to eat apples.\",\n",
        "        \"label\" : 5.0\n",
        "    },\n",
        "    {\n",
        "        \"sentence1\" :\"I like to eat apples.\",\n",
        "        \"sentence2\" :\"I like to eat bananas.\",\n",
        "        \"label\" : 4.0\n",
        "    },\n",
        "]\n",
        "\n",
        "train_predictions = [predict_similarity([sentence1, sentence2]) for sentence1, sentence2, _ in train_examples]\n",
        "# train_similarity_scores = [cosine_similarity(pred[0], pred[1]) for pred in train_predictions]\n",
        "# train_ground_truth_scores = [label for _, _, label in train_examples]\n",
        "\n",
        "# # Calculate train accuracy (you can use different metrics as needed)\n",
        "# train_accuracy = accuracy_score(train_ground_truth_scores, train_similarity_scores)\n",
        "# print(f\"Train Accuracy: {train_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate test accuracy\n",
        "test_accuracy = 0\n",
        "for test_data, test_label in tqdm(validation_dataloader):\n",
        "        \n",
        "            test_data['input_ids'] = test_data['input_ids'].to(device)\n",
        "            test_data['attention_mask'] = test_data['attention_mask'].to(device)\n",
        "        \n",
        "            test_data = collate_fn(test_data)\n",
        "        \n",
        "            with torch.no_grad():\n",
        "                output = [model(feature) for feature in test_data]\n",
        "        \n",
        "            test_accuracy += np.sum(np.round(output) == test_label.to(device).cpu().numpy())\n",
        "\n",
        "print(\"Test Accuracy: {}\".format(test_accuracy/len(val_ds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model for further use\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "# load model\n",
        "model = BertForSTS()\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# load test data\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "test_data.head()\n",
        "\n",
        "# convert test data to list\n",
        "test_data_list = test_data.values.tolist()\n",
        "\n",
        "# test_data_list : provide demo\n",
        "test_data_list = [['I like to eat apples.', 'I like to eat apples.'],\n",
        "                  ['I like to eat apples.', 'I like to eat bananas.'],\n",
        "                  ['I like to eat apples.', 'I like to eat oranges.'],\n",
        "                  ['I like to eat apples.', 'I like to eat pears.'],\n",
        "                  ['I like to eat apples.', 'I like to eat grapes.'],\n",
        "                  ['I like to eat apples.', 'I like to eat watermelons.'],\n",
        "                  ['I like to eat apples.', 'I like to eat pineapples.'],\n",
        "                  ['I like to eat apples.', 'I like to eat strawberries.'],\n",
        "                  ['I like to eat apples.', 'I like to eat blueberries.'],\n",
        "                  ['I like to eat apples.', 'I like to eat raspberries.'],\n",
        "                  ['I like to eat apples.', 'I like to eat blackberries.'],\n",
        "                  ]\n",
        "\n",
        "# predict similarity score\n",
        "similarity_score = []\n",
        "for i in tqdm(test_data_list):\n",
        "    test_data = tokenizer([i[0], i[1]], padding='max_length',\n",
        "                          max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "    test_data = collate_fn(test_data)\n",
        "    with torch.no_grad():\n",
        "        output = [model(feature) for feature in test_data]\n",
        "    similarity_score.append(output[0].item())\n",
        "\n",
        "# add similarity score to test data\n",
        "test_data['similarity_score'] = similarity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_examples = [\n",
        "    {\n",
        "        \"sentence1\" :\"To simulate the behaviour of portions of the desired software product.\",   \n",
        "        \"sentence2\" :\"High risk problems are address in the prototype program to make sure that the program is feasible.  A prototype may also be used to show a company that the software can be possibly programmed.\",\n",
        "        \"label\" : 1.0\n",
        "    },\n",
        "    {\n",
        "        \"sentence1\" :\"I like to eat apples.\",\n",
        "        \"sentence2\" :\"I like to eat bananas.\",\n",
        "        \"label\" : 0.8\n",
        "    },\n",
        "]\n",
        "\n",
        "test_labels = [example['label'] for example in test_examples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_predictions = [0.8, 0.9]\n",
        "\n",
        "test_labels = [0.7, 0.8]\n",
        "\n",
        "# calculate accuracy mse\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "nEgMWBU7fzXh",
        "outputId": "457e73b8-bd3f-4beb-db73-eb38fa9427cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-bdac8e6a-7776-4de3-880c-3bc4a674f798\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.065612</td>\n",
              "      <td>0.050554</td>\n",
              "      <td>0:05:37</td>\n",
              "      <td>0:00:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.040409</td>\n",
              "      <td>0.042842</td>\n",
              "      <td>0:05:38</td>\n",
              "      <td>0:00:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.036535</td>\n",
              "      <td>0.041365</td>\n",
              "      <td>0:05:31</td>\n",
              "      <td>0:00:28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdac8e6a-7776-4de3-880c-3bc4a674f798')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-e0351047-2a8d-4e84-8656-1f73b138683d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0351047-2a8d-4e84-8656-1f73b138683d')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-e0351047-2a8d-4e84-8656-1f73b138683d button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bdac8e6a-7776-4de3-880c-3bc4a674f798 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bdac8e6a-7776-4de3-880c-3bc4a674f798');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss Training Time Validation Time\n",
              "epoch                                                          \n",
              "1           0.065612     0.050554       0:05:37         0:00:29\n",
              "2           0.040409     0.042842       0:05:38         0:00:28\n",
              "3           0.036535     0.041365       0:05:31         0:00:28"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a DataFrame from our training statistics\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Display the table\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ahIyP4zsXp",
        "outputId": "920b9b8f-3e0e-43c8-b94a-ea48d1bd5096"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset stsb_multi_mt (/root/.cache/huggingface/datasets/stsb_multi_mt/en/1.0.0/a5d260e4b7aa82d1ab7379523a005a366d9b124c76a5a5cf0c4c5365458b0ba9)\n"
          ]
        }
      ],
      "source": [
        "test_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\", split=\"test\")\n",
        "\n",
        "# Prepare the data\n",
        "first_sent = [i['sentence1'] for i in test_dataset]\n",
        "second_sent = [i['sentence2'] for i in test_dataset]\n",
        "full_text = [[str(x), str(y)] for x,y in zip(first_sent, second_sent)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD7oPneMkUhe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "def predict_similarity(sentence_pair):\n",
        "  test_input = tokenizer(sentence_pair, padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\").to(device)\n",
        "  test_input['input_ids'] = test_input['input_ids']\n",
        "  test_input['attention_mask'] = test_input['attention_mask']\n",
        "  del test_input['token_type_ids']\n",
        "  output = model(test_input)\n",
        "  sim = torch.nn.functional.cosine_similarity(output[0], output[1], dim=0).item()\n",
        "\n",
        "  return sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-lGkcofz6hS",
        "outputId": "cd6916a5-f016-43e0-a79a-f2bc732c3447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: A cat is walking around a house.\n",
            "Sentence 2: A woman is peeling potato.\n",
            "Predicted similarity score: 0.05\n"
          ]
        }
      ],
      "source": [
        "example_1 = full_text[100]\n",
        "print(f\"Sentence 1: {example_1[0]}\")\n",
        "print(f\"Sentence 2: {example_1[1]}\")\n",
        "print(f\"Predicted similarity score: {round(predict_similarity(example_1), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViwfU0M2DOgh",
        "outputId": "3764ce32-b6bb-4866-fd9b-e1fc30aa7322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: Two men are playing football.\n",
            "Sentence 2: Two men are practicing football.\n",
            "Predicted similarity score: 0.73\n"
          ]
        }
      ],
      "source": [
        "example_2 = full_text[130]\n",
        "print(f\"Sentence 1: {example_2[0]}\")\n",
        "print(f\"Sentence 2: {example_2[1]}\")\n",
        "print(f\"Predicted similarity score: {round(predict_similarity(example_2), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xJHduVK9Yo7",
        "outputId": "1ee42126-8147-4cfe-ea62-40df3af26e53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Two men are playing football.', 'Two men are practicing football.']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGn-H7ARDnBG",
        "outputId": "e6afe442-fd07-4461-8fbe-2bb4eb49bce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: It varies by the situation.\n",
            "Sentence 2: This varies by institution.\n",
            "Predicted similarity score: 0.6\n"
          ]
        }
      ],
      "source": [
        "example_3 = full_text[812]\n",
        "print(f\"Sentence 1: {example_3[0]}\")\n",
        "print(f\"Sentence 2: {example_3[1]}\")\n",
        "print(f\"Predicted similarity score: {round(predict_similarity(example_3), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XovRH0VkXXs"
      },
      "source": [
        "### Last but not least, save your model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om3wskAQkaJP"
      },
      "outputs": [],
      "source": [
        "PATH = 'bert-sts.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCe1I2soj-Kj",
        "outputId": "c6d620b0-2640-403a-86c7-7b4485114345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSTS(\n",
              "  (bert): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "  (pooling_layer): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  (sts_bert): SentenceTransformer(\n",
              "    (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n",
              "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In order to load the model\n",
        "# First, you have to create an instance of the model's class\n",
        "# And use the saving path for the loading\n",
        "# Don't forget to set the model to the evaluation state using .eval()\n",
        "model = BertForSTS()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuJ_lVa34CPA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

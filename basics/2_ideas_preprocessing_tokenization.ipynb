{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Three main NLP Techniques__\n",
    "\n",
    "1. Rules & Heuristics (Regex etc.)\n",
    "2. Machine Learning\n",
    "3. Deep Learning\n",
    "   \n",
    "##### __NLP Tasks__\n",
    "\n",
    "1. Text Classification\n",
    "2. Text Similiraty(Pegasus)\n",
    "3. Information Extraction(Keword Based)\n",
    "4. Information Retrieval\n",
    "5. Chat Bots[FAQ Bot, Flow-Based Bot, Open-Ended Bot]\n",
    "6. Machine Translation{RNN- Encode/Decoder}\n",
    "7. Language Modeling(Sentence Auto Comlete Task)\n",
    "8. Text Summarization\n",
    "9. Topic Modeling(Retrieving Abstract Topics)\n",
    "10. Voice Assistants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before Moving into NLP\n",
    "1. https://youtu.be/m_CooIRM3UI -> Cosine similarity, cosine distance explained | Math, Statistics for data science, machine learning\n",
    "2. https://youtu.be/7kLi8u2dJz0?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO -> What is BERT? | Deep Learning Tutorial 46 (Tensorflow, Keras & Python)\n",
    "3. https://youtu.be/hOCDJyZ6quA?list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO -> Text Classification Using BERT & Tensorflow | Deep Learning Tutorial 47 (Tensorflow, Keras & Python)\n",
    "4. https://youtu.be/HdlDYng8g9s -> Machine Learning Tutorial Python - 16: Hyper parameter Tuning (GridSearchCV)\n",
    "5. https://youtu.be/JnlM4yLFNuo -> Handling imbalanced dataset in machine learning | Deep Learning Tutorial 21 (Tensorflow2.0 & Python)\n",
    "6. https://youtu.be/2osIZ-dSPGE -> Precision, Recall, F1 score, True Positive|Deep Learning Tutorial 19 (Tensorflow2.0, Keras & Python)\n",
    "7. Done : https://youtu.be/hQwFeIupNP0 -> What is Word2Vec? A Simple Explanation | Deep Learning Tutorial 41 (Tensorflow, Keras & Python)\n",
    "8. Done: https://youtu.be/sZGuyTLjsco -> Converting words to numbers, Word Embeddings | Deep Learning Tutorial 39 (Tensorflow & Python)\n",
    "9. https://www.firstlanguage.in/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training Of NLP Problems**\n",
    "1. https://youtu.be/3XiJrn_8F9Q\n",
    "2. Illustrated Guide to Transformers Neural Network: A step by step explanation : https://youtu.be/4Bdc55j80l8\n",
    "7. Tutorial 1-Transformer And Bert Implementation With Huggingface : https://youtu.be/DkzbCJtFvqM\n",
    "8. Custom Training Question Answer Model Using Transformer BERT : https://youtu.be/V1-Hm2rNkik"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Text Preprocessing Using (spaCy & NLTK)__\n",
    "\n",
    "##### __Spacy__\n",
    "1. OOP Based\n",
    "2. Provides most efficient NLP algorithm for a given task. Hence if you care about the end result, go with Spacy.\n",
    "3. It is a user friendly.\n",
    "4. Perfect for app developers.\n",
    "5. Spacy is new library and has a very active user community.\n",
    "\n",
    "##### __NLTK__\n",
    "1. Mainly String Processing\n",
    "2. Provides access to many algorithms. If you care about specific algo and customizations go with NLTK\n",
    "3. It is also user friendly but probably less user friendly compared to Spacy\n",
    "4. Perfect for researchers.\n",
    "5. NLTK is old libraray, User community as active as Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "data = \"True, Dr. the impact of external factors cannot be denied, but to blame high prices wholly on them would be wrong. A major part of it can be blamed on the government's failure to prioritise the interest of consumers and ensure good, accountable governance.\"\n",
    "\n",
    "bn_data = \"কক্সবাজারের চকরিয়ায় পিকআপচাপায় ছয় ভাই হত্যাকাণ্ডের রায়ে সন্তুষ্ট হতে পারেননি তাঁদের মা মৃণালিনী সুশীল। তিনি বলেন, ‘রায়ে আমার চাওয়া পূরণ হয়নি। এই হত্যাকাণ্ড নিয়ে আমি যে প্রশ্নের উত্তর চেয়েছিলাম, সেটি পাইনি। এ জন্য আমি আপিল করব।’ গত রোববার দুপুরে কক্সবাজার জেলা ও দায়রা জজ মোহাম্মদ ইসমাইল চকরিয়ায় পিকআপচাপায় ছয় ভাইয়ের মৃত্যুর ঘটনায় রায় ঘোষণা করেন। ৳ ১০০০ টাকা\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy Language Model Installation**\n",
    "\n",
    "- python3 -m spacy download en_core_web_sm\n",
    "- python3 -m spacy download en_core_web_md\n",
    "- python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "কক্সবাজারের False False\n",
      "চকরিয়ায় False False\n",
      "পিকআপচাপায় False False\n",
      "ছয় False False\n",
      "ভাই False False\n",
      "হত্যাকাণ্ডের False False\n",
      "রায়ে False False\n",
      "সন্তুষ্ট False False\n",
      "হতে False False\n",
      "পারেননি False False\n",
      "তাঁদের False False\n",
      "মা False False\n",
      "মৃণালিনী False False\n",
      "সুশীল False False\n",
      "। False False\n",
      "তিনি False False\n",
      "বলেন False False\n",
      ", False False\n",
      "‘ False False\n",
      "রায়ে False False\n",
      "আমার False False\n",
      "চাওয়া False False\n",
      "পূরণ False False\n",
      "হয়নি False False\n",
      "। False False\n",
      "এই False False\n",
      "হত্যাকাণ্ড False False\n",
      "নিয়ে False False\n",
      "আমি False False\n",
      "যে False False\n",
      "প্রশ্নের False False\n",
      "উত্তর False False\n",
      "চেয়েছিলাম False False\n",
      ", False False\n",
      "সেটি False False\n",
      "পাইনি False False\n",
      "। False False\n",
      "এ False False\n",
      "জন্য False False\n",
      "আমি False False\n",
      "আপিল False False\n",
      "করব False False\n",
      "। False False\n",
      "’ False False\n",
      "গত False False\n",
      "রোববার False False\n",
      "দুপুরে False False\n",
      "কক্সবাজার False False\n",
      "জেলা False False\n",
      "ও False False\n",
      "দায়রা False False\n",
      "জজ False False\n",
      "মোহাম্মদ False False\n",
      "ইসমাইল False False\n",
      "চকরিয়ায় False False\n",
      "পিকআপচাপায় False False\n",
      "ছয় False False\n",
      "ভাইয়ের False False\n",
      "মৃত্যুর False False\n",
      "ঘটনায় False False\n",
      "রায় False False\n",
      "ঘোষণা False False\n",
      "করেন False False\n",
      "। False False\n",
      "৳ True False\n",
      "১০০০ False True\n",
      "টাকা False False\n"
     ]
    }
   ],
   "source": [
    "# Sentence and Word Tokenization using OOP\n",
    "nlp = spacy.load('en_core_web_sm') # Load the trained language model\n",
    "doc = nlp(data)\n",
    "# for sentence in doc.sents:\n",
    "#     for word in sentence:\n",
    "#         print(word.text)\n",
    "\n",
    "# For Bengali text\n",
    "nlp = spacy.blank('bn') # Load the trained language model\n",
    "doc = nlp(bn_data)\n",
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      ",\n",
      "Dr.\n",
      "the\n",
      "impact\n",
      "of\n",
      "external\n",
      "factors\n",
      "can\n",
      "not\n",
      "be\n",
      "denied\n",
      ",\n",
      "but\n",
      "to\n",
      "blame\n",
      "high\n",
      "prices\n",
      "wholly\n",
      "on\n",
      "them\n",
      "would\n",
      "be\n",
      "wrong\n",
      ".\n",
      "A\n",
      "major\n",
      "part\n",
      "of\n",
      "it\n",
      "can\n",
      "be\n",
      "blamed\n",
      "on\n",
      "the\n",
      "government\n",
      "'s\n",
      "failure\n",
      "to\n",
      "prioritise\n",
      "the\n",
      "interest\n",
      "of\n",
      "consumers\n",
      "and\n",
      "ensure\n",
      "good\n",
      ",\n",
      "accountable\n",
      "governance\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mijanur/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# punkt is a pre-trained model for sentence tokenization\n",
    "# nltk.download()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "# Sentence and Word Tokenization using nltk\n",
    "def tokenize(data):\n",
    "    for sentence in sent_tokenize(data):\n",
    "        for word in word_tokenize(sentence):\n",
    "            print(word)\n",
    "\n",
    "tokenize(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *__Tokenization in Spacy__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "two\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "and\n",
      "buy\n",
      "$\n",
      "30\n",
      "\"\n",
      "\" ==> index:  0 is_alpha:  False is_stop:  False is_punct:  True is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  False\n",
      "Let ==> index:  1 is_alpha:  True is_stop:  False is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  False\n",
      "'s ==> index:  2 is_alpha:  False is_stop:  True is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  True\n",
      "two ==> index:  3 is_alpha:  True is_stop:  True is_punct:  False is_space:  False like_num:  True is_currency:  False is_digit:  False is_lower:  True\n",
      "go ==> index:  4 is_alpha:  True is_stop:  True is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  True\n",
      "to ==> index:  5 is_alpha:  True is_stop:  True is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  True\n",
      "N.Y. ==> index:  6 is_alpha:  False is_stop:  False is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  False\n",
      "! ==> index:  7 is_alpha:  False is_stop:  False is_punct:  True is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  False\n",
      "and ==> index:  8 is_alpha:  True is_stop:  True is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  True\n",
      "buy ==> index:  9 is_alpha:  True is_stop:  False is_punct:  False is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  True\n",
      "$ ==> index:  10 is_alpha:  False is_stop:  False is_punct:  False is_space:  False like_num:  False is_currency:  True is_digit:  False is_lower:  False\n",
      "30 ==> index:  11 is_alpha:  False is_stop:  False is_punct:  False is_space:  False like_num:  True is_currency:  False is_digit:  True is_lower:  False\n",
      "\" ==> index:  12 is_alpha:  False is_stop:  False is_punct:  True is_space:  False like_num:  False is_currency:  False is_digit:  False is_lower:  False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "# As here is blank, So we need to add the pipeline components. that will be adedd next\n",
    "\n",
    "data = '''\"Let's two go to N.Y.! and buy $30\"''' # N.Y. is a single token here because of the period in between N and Y \n",
    "# and the exclamation mark at the end of the sentence.\n",
    "# Prefix -> Exception -> Infix -> Suffix .... -> Done\n",
    "doc = nlp(data)\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# print(type(doc))\n",
    "# print(type(nlp))\n",
    "\n",
    "# token0 = doc[3]\n",
    "# print(token0.text)\n",
    "# print(dir(token0))\n",
    "\n",
    "# print(token0.like_num)\n",
    "\n",
    "for token in doc:\n",
    "    # print(token.text, token.pos_, token.dep_, token.lemma_, token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(token, \"==> \"\n",
    "          \"index: \", token.i,\n",
    "          \"is_alpha: \", token.is_alpha,\n",
    "          \"is_stop: \", token.is_stop,\n",
    "          \"is_punct: \", token.is_punct,\n",
    "          \"is_space: \", token.is_space,\n",
    "          \"like_num: \", token.like_num,\n",
    "          \"is_currency: \", token.is_currency,\n",
    "          \"is_digit: \", token.is_digit,\n",
    "          \"is_lower: \", token.is_lower,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shafiquil@gmail.com', 'rajuil@gmail.com', 'shafiquil@kohli.com', 'joe@koroothli.com']\n"
     ]
    }
   ],
   "source": [
    "# Extracting data from text file\n",
    "\n",
    "with open('students.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# print(data)\n",
    "\n",
    "# converting into single text \n",
    "data = ' '.join(data)\n",
    "# ' '.join(data) -> means join all the elements of data list with a space in between\n",
    "# print(data)\n",
    "\n",
    "# Sentence and Word Tokenization using spacy\n",
    "doc = nlp(data)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "print(emails)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Special Rules in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gim', 'me', 'double', 'cheese', 'burgers', 'and', 'fries', 'to', 'go', 'please', '.']\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"gimme double cheese burgers and fries to go please.\")\n",
    "# tokens = [token.text for token in doc]\n",
    "# print(tokens)\n",
    "\n",
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "# ORTH -> Orthography (spelling, punctuation, etc.)\n",
    "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
    "doc = nlp(\"gimme double cheese burgers and fries to go please.\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Checking Pipeline of NLP__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer']\n",
      "['sentencizer']\n",
      "Dr. Alex Smith chaired first board meeting at Google.\n",
      "He paid $1.3 million to buy U.S. startup last year.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Alex Smith chaired first board meeting at Google. He paid $1.3 million to buy U.S. startup last year.\")\n",
    "\n",
    "# for sent in doc.sents:\n",
    "#     print(sent)\n",
    "\n",
    "# no pipe names means no pipeline components have been added to the model\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Pipeline -> A pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.    \n",
    "# add a pipeline component\n",
    "\n",
    "# nlp.add_pipe('sentencizer')\n",
    "# sentencizer -> The sentencizer is a pipeline component that splits sentences on punctuation like ., ! and ?.\n",
    "# print(nlp.pipe_names)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. Find all the links from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.data.gov/', 'http://www.science', 'http://data.gov.uk/.', 'http://www3.norc.org/gss+website/', 'http://www.europeansocialsurvey.org/.']\n"
     ]
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc =  nlp(text)\n",
    "links = [token.text for token in doc if token.like_url]\n",
    "print(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Figure out all transactions from this text with amount and currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ two\n",
      "€ 500\n"
     ]
    }
   ],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "doc =  nlp(transactions)\n",
    "for token in doc:\n",
    "    if token.is_currency and doc[token.i - 1].like_num:\n",
    "        print(token.text, doc[token.i - 1].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

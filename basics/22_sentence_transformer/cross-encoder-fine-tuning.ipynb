{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sentence Transformers, a \"CrossEncoder\" is a type of model architecture that is designed to encode and compare pairs of sentences simultaneously. Unlike the \"Siamese\" architecture, where the two sentences are encoded separately and then their representations are compared, the CrossEncoder processes both sentences together in a single forward pass.\n",
    "\n",
    "The CrossEncoder architecture is commonly used for tasks that involve comparing pairs of sentences, such as sentence similarity, semantic textual similarity, paraphrase identification, and natural language inference. It's especially useful when you want to calculate a single similarity score or label for the entire sentence pair.\n",
    "\n",
    "Here's a high-level overview of how a CrossEncoder works:\n",
    "\n",
    "1. **Input Encoding:** The two sentences in the pair are tokenized and encoded together as a single input. They are processed through a pre-trained transformer-based model, such as BERT, RoBERTa, or other variants, which produces a fixed-length vector representation for the entire sentence pair.\n",
    "\n",
    "2. **Similarity Calculation:** The encoded sentence pair representation is then passed through one or more fully connected layers or other transformation functions to calculate a similarity score. This score indicates how similar the two sentences are in terms of their semantic meaning, with higher scores indicating higher similarity.\n",
    "\n",
    "3. **Training:** During training, CrossEncoder models are often fine-tuned using supervised learning on tasks that require sentence pair comparisons. The model is trained with labeled data, where each sentence pair is associated with a similarity score or a binary label (e.g., paraphrase/non-paraphrase).\n",
    "\n",
    "The CrossEncoder architecture is particularly useful when you need a single model that can handle multiple types of similarity-related tasks, as it can provide efficient and accurate sentence pair comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the NLI task in NLP?**\n",
    "\n",
    "Natural Language Inference (NLI) is the task of determining whether the given “hypothesis” logically follows from the “premise”. In layman's terms, you need to understand whether the hypothesis is true, while the premise is your only knowledge about the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiNLI\n",
    " The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen. \n",
    "\n",
    "**Examples**\n",
    "\n",
    "Premise \t                                            Label \t    Hypothesis\n",
    "\n",
    "The Old One always comforted Ca'daan, except today. \tneutral \tCa'daan knew the Old One very well.\n",
    "\n",
    "Your gift is appreciated by each and every student \n",
    "who will benefit from your generosity.                  neutral \tHundreds of students will benefit from your generosity.\n",
    "\n",
    "yes now you know if if everybody like in August when \n",
    "everybody's on vacation or something we can dress \n",
    "a little more casual or \t                            contradiction \tAugust is a black out month for vacations in the company.\n",
    "\n",
    "\n",
    "At the other end of Pennsylvania Avenue, people began \n",
    "to line up for a White House tour. \t                    entailment \tPeople formed a line at the end of Pennsylvania Avenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers   \n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goo Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My personal traning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/my_new_ds_train_eval_test.zip /content/drive/My\\ Drive/Colab\\ Notebooks/ML\\ Project\\ 2/my_new_ds_train_eval_test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NLI Traning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/data_nlp_nli.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "      <th>\"avg_score\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is NLP?</td>\n",
       "      <td>NLP is the field of artificial intelligence th...</td>\n",
       "      <td>NLP is a form of advanced image recognition te...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is NLP?</td>\n",
       "      <td>NLP involves the study and application of comp...</td>\n",
       "      <td>NLP is an ancient form of divination used to p...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is NLP?</td>\n",
       "      <td>NLP is a multidisciplinary field that combines...</td>\n",
       "      <td>NLP is a synonym for computer programming and ...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is NLP?</td>\n",
       "      <td>NLP refers to the technology and methods used ...</td>\n",
       "      <td>NLP is a type of gardening technique used to c...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is NLP?</td>\n",
       "      <td>NLP is an area of research that focuses on bri...</td>\n",
       "      <td>NLP is a subfield of oceanography dealing with...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What are the limitations of machine translatio...</td>\n",
       "      <td>Machine translation in NLP faces various limit...</td>\n",
       "      <td>Machine translation in NLP has no limitations ...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>What are the limitations of machine translatio...</td>\n",
       "      <td>Machine translation in NLP is not flawless and...</td>\n",
       "      <td>Machine translation in NLP is flawless and has...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>What are the limitations of machine translatio...</td>\n",
       "      <td>Limitations of machine translation in NLP incl...</td>\n",
       "      <td>Machine translation in NLP is perfect and has ...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>What are the limitations of machine translatio...</td>\n",
       "      <td>Machine translation in NLP encounters limitati...</td>\n",
       "      <td>Machine translation in NLP is highly accurate ...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>What are the limitations of machine translatio...</td>\n",
       "      <td>In NLP, machine translation has notable limita...</td>\n",
       "      <td>Machine translation in NLP is unlimited in acc...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0                                         What is NLP?   \n",
       "1                                         What is NLP?   \n",
       "2                                         What is NLP?   \n",
       "3                                         What is NLP?   \n",
       "4                                         What is NLP?   \n",
       "..                                                 ...   \n",
       "109  What are the limitations of machine translatio...   \n",
       "110  What are the limitations of machine translatio...   \n",
       "111  What are the limitations of machine translatio...   \n",
       "112  What are the limitations of machine translatio...   \n",
       "113  What are the limitations of machine translatio...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    NLP is the field of artificial intelligence th...   \n",
       "1    NLP involves the study and application of comp...   \n",
       "2    NLP is a multidisciplinary field that combines...   \n",
       "3    NLP refers to the technology and methods used ...   \n",
       "4    NLP is an area of research that focuses on bri...   \n",
       "..                                                 ...   \n",
       "109  Machine translation in NLP faces various limit...   \n",
       "110  Machine translation in NLP is not flawless and...   \n",
       "111  Limitations of machine translation in NLP incl...   \n",
       "112  Machine translation in NLP encounters limitati...   \n",
       "113  In NLP, machine translation has notable limita...   \n",
       "\n",
       "                                            hypothesis          label subject  \\\n",
       "0    NLP is a form of advanced image recognition te...  contradiction     nlp   \n",
       "1    NLP is an ancient form of divination used to p...  contradiction     nlp   \n",
       "2    NLP is a synonym for computer programming and ...  contradiction     nlp   \n",
       "3    NLP is a type of gardening technique used to c...  contradiction     nlp   \n",
       "4    NLP is a subfield of oceanography dealing with...  contradiction     nlp   \n",
       "..                                                 ...            ...     ...   \n",
       "109  Machine translation in NLP has no limitations ...  contradiction     nlp   \n",
       "110  Machine translation in NLP is flawless and has...  contradiction     nlp   \n",
       "111  Machine translation in NLP is perfect and has ...  contradiction     nlp   \n",
       "112  Machine translation in NLP is highly accurate ...  contradiction     nlp   \n",
       "113  Machine translation in NLP is unlimited in acc...  contradiction     nlp   \n",
       "\n",
       "      \"avg_score\"  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "..            ...  \n",
       "109           0.0  \n",
       "110           0.0  \n",
       "111           0.0  \n",
       "112           0.0  \n",
       "113           0.0  \n",
       "\n",
       "[114 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#As dataset, we use SNLI + MultiNLI\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logger.info(\"Read AllNLI train dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        label_id = label2int[row['label']]\n",
    "        if row['split'] == 'train':\n",
    "            train_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "        else:\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "#     df_x = pd.read_csv(fIn, sep='\\t', names=['sentence1', 'sentence2','label_id'], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    df_x = pd.read_csv(fIn, sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_allnli-' + \\\n",
    "    datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our CrossEncoder model. We use distilroberta-base as basis and setup it up to predict 3 labels\n",
    "model = CrossEncoder('distilroberta-base', num_labels=len(label2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "# During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
    "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(\n",
    "    dev_samples, name='AllNLI-dev')\n",
    "\n",
    "\n",
    "# 10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But it takes too time like 20 hours for traning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=10000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity Between two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# this is the best model for STSb dataset\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\n",
    "\"\"\"\n",
    "text2 = \"\"\"\n",
    "Gradient descent (GD) is not an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict([(text1,text2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use already fine-tuned Cross-Encoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NLI trained Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    (\"good public policy should make society healthier happier  safer and more productive\", \"scoial sciencists should be able to help us understand the world better\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_n = model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert computed liast of list equal scores_hugg to simple labels\n",
    "label_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "# original scores\n",
    "print(scores_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easy reading: convert list to singular label\n",
    "labels = [label_mapping[score_max] for score_max in scores_n.argmax(axis=1)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. STSb trained Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "model = CrossEncoder('cross-encoder/stsb-distilroberta-base',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_n = model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btach_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Cross Encoder Model on Custom DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "num_epochs = 3\n",
    "model_save_path = 'output/my_training_ds-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross encoder model\n",
    "my_model = CrossEncoder(\"distilroberta-base\", num_labels=len(label2int), max_length=512)\n",
    "\n",
    "# distilroberta-base is a smaller version of RoBERTa-base. It has 6 layers, 768 hidden size, 12 attention heads and about 82M parameters.\n",
    "\n",
    "# distilroberta-base is a smaller version of RoBERTa-base. It has 6 layers, 768 hidden size, 12 attention heads and about 82M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inplace = True means we are modifying the original dataframe, not creating a new one and assigning it to df again \n",
    "df.drop('question', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String to Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting label to numeric\n",
    "df['label'].replace(label2int, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split into train_df, val_df, test_df\n",
    "train_df,  test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df = test_df.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding data as input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_my = []\n",
    "test_samples = []\n",
    "\n",
    "# for i in range(len(train_df)):\n",
    "#     train_samples_my.append(InputExample(texts=[train_df.iloc[i]['question'], train_df.iloc[i]['answer']], label=train_df.iloc[i]['label']))\n",
    "\n",
    "# use anither way without for loop\n",
    "train_samples_my = [InputExample(texts=[train_df.iloc[i]['text'], train_df.iloc[i]['hypothesis']], label=train_df.iloc[i]['label']) for i in range(len(train_df))]\n",
    "\n",
    "test_samples = [InputExample(texts=[test_df.iloc[i]['text'], test_df.iloc[i]['hypothesis']], label=test_df.iloc[i]['label']) for i in range(len(test_df))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_my = DataLoader(train_samples_my, shuffle=True,batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_my = CESoftmaxAccuracyEvaluator.from_input_examples(test_samples, name='my_training_ds_evaluator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = math.ceil(len(train_dataloader_my) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "# warmup steps is a parameter that controls the learning rate during the first few epochs of training.\n",
    "# During the warm-up steps, the learning rate is linearly increased from 0 to the specified learning rate.\n",
    "# After the warm-up phase, the learning rate is decreasing again during training.\n",
    "# This is a common technique in transfer learning, especially for fine-tuning BERT models.\n",
    "\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CrossEntropyLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m      2\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m my_model\u001b[39m.\u001b[39mfit(train_dataloader\u001b[39m=\u001b[39mtrain_dataloader_my,\n\u001b[1;32m      4\u001b[0m              evaluator\u001b[39m=\u001b[39mevaluator_my,\n\u001b[1;32m      5\u001b[0m              epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m              save_best_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     10\u001b[0m              output_path\u001b[39m=\u001b[39mmodel_save_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CrossEntropyLoss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "my_model.fit(train_dataloader=train_dataloader_my,\n",
    "             evaluator=evaluator_my,\n",
    "             epochs=1,\n",
    "             evaluation_steps=2000,\n",
    "             warmup_steps=warmup_steps,\n",
    "             loss_fct=loss_fct,\n",
    "             save_best_model=True,\n",
    "             optimizer_class=AdamW,\n",
    "             optimizer_params={'lr': 2e-5},\n",
    "             output_path=model_save_path)\n",
    "\n",
    "# evaluation_steps means how often the model should be evaluated on the dev set.\n",
    "# save_best_model=True means that only the best model is saved on disk (correctly classified most dev samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load my pre-trained cross-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reloaded = CrossEncoder(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### eval on test set for single value outcome\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(test_samples, name='eval_test')\n",
    "evaluator(my_model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. My Training Dataset trained Cross Encoder(distilroberta-base) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_my = my_model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert computed liast of list equal scores_hugg to simple labels\n",
    "label_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "labels = [label_mapping[score_max] for score_max in scores_my.argmax(axis=1)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contradiction': 9, 'neutral': 8}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli = ['contradiction', 'contradiction', 'contradiction', 'neutral',\n",
    "       'neutral', 'contradiction', 'neutral', 'neutral', 'neutral', 'contradiction']\n",
    "\n",
    "# map labels to IDs\n",
    "label_map = {label: i for i, label in enumerate(nli)}\n",
    "label_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Sentence Transformers, a \"CrossEncoder\" is a type of model architecture that is designed to encode and compare pairs of sentences simultaneously. Unlike the \"Siamese\" architecture, where the two sentences are encoded separately and then their representations are compared, the CrossEncoder processes both sentences together in a single forward pass.\n",
    "\n",
    "The CrossEncoder architecture is commonly used for tasks that involve comparing pairs of sentences, such as sentence similarity, semantic textual similarity, paraphrase identification, and natural language inference. It's especially useful when you want to calculate a single similarity score or label for the entire sentence pair.\n",
    "\n",
    "Here's a high-level overview of how a CrossEncoder works:\n",
    "\n",
    "1. **Input Encoding:** The two sentences in the pair are tokenized and encoded together as a single input. They are processed through a pre-trained transformer-based model, such as BERT, RoBERTa, or other variants, which produces a fixed-length vector representation for the entire sentence pair.\n",
    "\n",
    "2. **Similarity Calculation:** The encoded sentence pair representation is then passed through one or more fully connected layers or other transformation functions to calculate a similarity score. This score indicates how similar the two sentences are in terms of their semantic meaning, with higher scores indicating higher similarity.\n",
    "\n",
    "3. **Training:** During training, CrossEncoder models are often fine-tuned using supervised learning on tasks that require sentence pair comparisons. The model is trained with labeled data, where each sentence pair is associated with a similarity score or a binary label (e.g., paraphrase/non-paraphrase).\n",
    "\n",
    "The CrossEncoder architecture is particularly useful when you need a single model that can handle multiple types of similarity-related tasks, as it can provide efficient and accurate sentence pair comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the NLI task in NLP?**\n",
    "\n",
    "Natural Language Inference (NLI) is the task of determining whether the given “hypothesis” logically follows from the “premise”. In layman's terms, you need to understand whether the hypothesis is true, while the premise is your only knowledge about the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiNLI\n",
    " The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen. \n",
    "\n",
    "**Examples**\n",
    "\n",
    "Premise \t                                            Label \t    Hypothesis\n",
    "\n",
    "The Old One always comforted Ca'daan, except today. \tneutral \tCa'daan knew the Old One very well.\n",
    "\n",
    "Your gift is appreciated by each and every student \n",
    "who will benefit from your generosity.                  neutral \tHundreds of students will benefit from your generosity.\n",
    "\n",
    "yes now you know if if everybody like in August when \n",
    "everybody's on vacation or something we can dress \n",
    "a little more casual or \t                            contradiction \tAugust is a black out month for vacations in the company.\n",
    "\n",
    "\n",
    "At the other end of Pennsylvania Avenue, people began \n",
    "to line up for a White House tour. \t                    entailment \tPeople formed a line at the end of Pennsylvania Avenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine Tuning SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers   \n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goo Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My personal traning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/my_new_ds_train_eval_test.zip /content/drive/My\\ Drive/Colab\\ Notebooks/ML\\ Project\\ 2/my_new_ds_train_eval_test.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download NLI Traning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "from sentence_transformers import LoggingHandler, util\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linux is an open-source operating system known...</td>\n",
       "      <td>Windows,a widely used operating system develop...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linux,with its command-line interface,provides...</td>\n",
       "      <td>Windows,unlike Linux,requires a graphical user...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linux distributions,such as Ubuntu,Fedora,and ...</td>\n",
       "      <td>Windows,being a commercial operating system,co...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Windows,despite being a widely used operating ...</td>\n",
       "      <td>Linux,with its frequent software updates and r...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linux,due to its lightweight and efficient des...</td>\n",
       "      <td>Windows,as a more resource-intensive operating...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Machine learning enables computers to learn fr...</td>\n",
       "      <td>Artificial Intelligence has transformed variou...</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Sorting algorithms are crucial for arranging d...</td>\n",
       "      <td>Data structures are fundamental for efficient ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Agile development emphasizes iterative and cus...</td>\n",
       "      <td>Software development methodologies like Agile ...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>React.js,a popular front-end framework,offers ...</td>\n",
       "      <td>Web development involves front-end and back-en...</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>HTTP,a stateless protocol,facilitates resource...</td>\n",
       "      <td>TCP/IP,the backbone of the internet,ensures re...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Linux is an open-source operating system known...   \n",
       "1    Linux,with its command-line interface,provides...   \n",
       "2    Linux distributions,such as Ubuntu,Fedora,and ...   \n",
       "3    Windows,despite being a widely used operating ...   \n",
       "4    Linux,due to its lightweight and efficient des...   \n",
       "..                                                 ...   \n",
       "259  Machine learning enables computers to learn fr...   \n",
       "260  Sorting algorithms are crucial for arranging d...   \n",
       "261  Agile development emphasizes iterative and cus...   \n",
       "262  React.js,a popular front-end framework,offers ...   \n",
       "263  HTTP,a stateless protocol,facilitates resource...   \n",
       "\n",
       "                                            hypothesis          label  \n",
       "0    Windows,a widely used operating system develop...  contradiction  \n",
       "1    Windows,unlike Linux,requires a graphical user...  contradiction  \n",
       "2    Windows,being a commercial operating system,co...  contradiction  \n",
       "3    Linux,with its frequent software updates and r...     entailment  \n",
       "4    Windows,as a more resource-intensive operating...     entailment  \n",
       "..                                                 ...            ...  \n",
       "259  Artificial Intelligence has transformed variou...  contradiction  \n",
       "260  Data structures are fundamental for efficient ...     entailment  \n",
       "261  Software development methodologies like Agile ...     entailment  \n",
       "262  Web development involves front-end and back-en...     entailment  \n",
       "263  TCP/IP,the backbone of the internet,ensures re...        neutral  \n",
       "\n",
       "[264 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#As dataset, we use SNLI + MultiNLI\n",
    "#Check if dataset exsist. If not, download and extract  it\n",
    "nli_dataset_path = 'datasets/AllNLI.tsv.gz'\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get('https://sbert.net/datasets/AllNLI.tsv.gz', nli_dataset_path)\n",
    "\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "logger.info(\"Read AllNLI train dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        label_id = label2int[row['label']]\n",
    "        if row['split'] == 'train':\n",
    "            train_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))\n",
    "        else:\n",
    "            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=label_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(nli_dataset_path, 'rt', encoding='utf8') as fIn:\n",
    "    df_x = pd.read_csv(fIn, sep='\\t', names=['sentence1', 'sentence2','label_id'], quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)['sentence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.head(1)['sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_allnli-' + \\\n",
    "    datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our CrossEncoder model. We use distilroberta-base as basis and setup it up to predict 3 labels\n",
    "model = CrossEncoder('distilroberta-base', num_labels=len(label2int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap train_samples, which is a list ot InputExample, in a pytorch DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_samples, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "# During training, we use CESoftmaxAccuracyEvaluator to measure the accuracy on the dev set.\n",
    "evaluator = CESoftmaxAccuracyEvaluator.from_input_examples(\n",
    "    dev_samples, name='AllNLI-dev')\n",
    "\n",
    "\n",
    "# 10% of train data for warm-up\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But it takes too time like 20 hours for traning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_dataloader=train_dataloader,\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=10000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use already fine-tuned Cross-Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "model_hugg = CrossEncoder('cross-encoder/nli-distilroberta-base', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    (\"good public policy should make society healthier happier  safer and more productive\", \"scoial sciencists should be able to help us understand the world better\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_hugg = model_hugg.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert computed liast of list equal scores_hugg to simple labels\n",
    "label_mapping = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "# original scores\n",
    "print(scores_hugg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easy reading: convert list to singular label\n",
    "labels = [label_mapping[score_max] for score_max in scores_hugg.argmax(axis=1)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_btach_size = 16\n",
    "num_epochs = 4\n",
    "model_save_path = 'output/training_stsbenchmark_continue_training-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Cross Encoder Model on Custom DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "num_epochs = 3\n",
    "model_save_path = 'output/my_training_ds-' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross encoder model\n",
    "my_model = CrossEncoder(\"distilroberta-base\", num_labels=len(label2int), max_length=512)\n",
    "\n",
    "# distilroberta-base is a smaller version of RoBERTa-base. It has 6 layers, 768 hidden size, 12 attention heads and about 82M parameters.\n",
    "\n",
    "# distilroberta-base is a smaller version of RoBERTa-base. It has 6 layers, 768 hidden size, 12 attention heads and about 82M parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\n",
    "    'contradiction': 0,\n",
    "    'entailment': 1,\n",
    "    'neutral': 2\n",
    "}\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        'sentence1': 'I like to eat apples.',\n",
    "        'sentence2': 'Apples are my favorite fruit.',\n",
    "        'label': 1\n",
    "    },\n",
    "    {\n",
    "        'sentence1': 'I like to eat apples.',\n",
    "        'sentence2': 'Apples are my favorite fruit.',\n",
    "        'label': 1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_my = []\n",
    "for i in range(len(rows)):\n",
    "    train_samples_my.append(InputExample(texts=[train_samples[i]['sentence1'], train_samples[i]['sentence2']], label=train_samples[i]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_my = DataLoader(train_samples_my, shuffle=True,batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_my = CESoftmaxAccuracyEvaluator.from_input_examples(test_samples, name='my-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
    "\n",
    "# warmup steps is a parameter that controls the learning rate during the first few epochs of training.\n",
    "# During the warm-up steps, the learning rate is linearly increased from 0 to the specified learning rate.\n",
    "# After the warm-up phase, the learning rate is decreasing again during training.\n",
    "# This is a common technique in transfer learning, especially for fine-tuning BERT models.\n",
    "\n",
    "\n",
    "logger.info(\"Warmup-steps: {}\".format(warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "my_model.fit(train_dataloader=train_dataloader,\n",
    "             evaluator=evaluator,\n",
    "             epochs=1,\n",
    "             evaluation_steps=2000,\n",
    "             warmup_steps=warmup_steps,\n",
    "             save_best_model=True,\n",
    "             output_path=model_save_path)\n",
    "# evaluation_steps means how often the model should be evaluated on the dev set.\n",
    "# save_best_model=True means that only the best model is saved on disk (correctly classified most dev samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

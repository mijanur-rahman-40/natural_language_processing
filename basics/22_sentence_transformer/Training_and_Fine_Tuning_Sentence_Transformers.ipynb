{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine Tuning Sentence Tranformer\n",
        "- https://huggingface.co/blog/how-to-train-sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q7ReLxHVXmUe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install sentence-transformers\n",
        "#  sentence-transformers is a python framework for state-of-the-art sentence, text and image embeddings. It is backed by the popular HuggingFace transformers library. It provides a simple interface for computing embeddings while hiding the complex machinery behind it. It also supports fine-tuning of embeddings models on custom datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bVcerIvmXkLY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mijanur/Desktop/AI/DL NLP/natural_language_processing/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-07-19 10:02:59.555506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-19 10:03:10.341662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import InputExample, SentenceTransformer\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "v7HC3hCGX9cU"
      },
      "outputs": [],
      "source": [
        "# Prepare your training data\n",
        "train_examples = []\n",
        "\n",
        "# Generate train examples with keywords and labels\n",
        "train_examples = []\n",
        "\n",
        "# premise,hypothesis, label\n",
        "\n",
        "# Example 1\n",
        "sentence1 = \"Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.\"\n",
        "sentence2 = \"Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.\"\n",
        "label = .61  # Similar sentences\n",
        "\n",
        "train_examples.append(InputExample(texts=[sentence1, sentence2], label=label))\n",
        "\n",
        "# # Example 2\n",
        "# sentence1 = \"Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.\"\n",
        "# sentence2 = \"Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression). Due to its importance and ease of implementation, this algorithm is usually taught at the beginning of almost all machine learning courses.\"\n",
        "# label = 0  # Similar sentences\n",
        "\n",
        "sentence1 = \"\"\"\n",
        "Bangladesh rose from 101st place in the January edition of the Henley Passport Index to 96th place now. Kosovo also holds the same place. Bangladeshi passport holders now have access to on-arrival visa options in 40 countries, according to the Henley and Partners, a London-based organization that released the most recent index of passports on Tuesday.  With its passport holders having access to on-arrival visa options in as many as 192 countries, Singapore presently occupies the top spot in the index.\n",
        "\"\"\"\n",
        "sentence2 = \"\"\"Bangladesh has climbed to the 96th position in Henley Passport Index, from its previous ranking of 101st in the January edition. It shares the position with Kosovo. The Henley and Partners, a London-based organisation, released the latest passport index on Tuesday, saying that the Bangladeshi passport holders now enjoy on-arrival visa facilities in 40 countries.  Singapore currently holds the top position in the index, with its passport holders enjoying on-arrival visa facilities in as many as 192 countries.\n",
        "\"\"\"\n",
        "label = 1\n",
        "train_examples.append(InputExample(texts=[sentence1, sentence2], label=label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate train examples with keywords and labels\n",
        "train_examples = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Home</td>\n",
              "      <td>fa fa-home</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>About</td>\n",
              "      <td>fa fa-info</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Contact</td>\n",
              "      <td>fa fa-phone</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Settings</td>\n",
              "      <td>fa fa-cog</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Logout</td>\n",
              "      <td>fa fa-sign-out</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question         context  label\n",
              "0      Home      fa fa-home      0\n",
              "1     About      fa fa-info      0\n",
              "2   Contact     fa fa-phone      0\n",
              "3  Settings       fa fa-cog      0\n",
              "4    Logout  fa fa-sign-out      0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_json('data.json')\n",
        "# lines = True means each line is a json object\n",
        "\n",
        "df.shape\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_examples.append(InputExample(texts=[df['question'][0], df['context'][0]], label=df['label'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEVHotYbcZWF",
        "outputId": "6103ffe8-3772-4d33-e490-dba3f2e9cddb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 2.32MB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 550/550 [00:00<00:00, 3.56MB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 414kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 679kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 691kB/s]\n",
            "Downloading pytorch_model.bin: 100%|██████████| 265M/265M [01:12<00:00, 3.66MB/s] \n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-base-nli-mean-tokens and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained Sentence Transformer model\n",
        "model_name = 'sentence-transformers/distilbert-base-nli-mean-tokens'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_hL6RG2AcbOO"
      },
      "outputs": [],
      "source": [
        "# Prepare your training data\n",
        "train_examples = train_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6I3WTlJycjy-"
      },
      "outputs": [],
      "source": [
        "# Tokenize and convert train examples to features\n",
        "train_features = tokenizer.batch_encode_plus(\n",
        "    [(example.texts[0], example.texts[1]) for example in train_examples],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "train_labels = torch.tensor([example.label for example in train_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "V48bJHe5clax"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning setup\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features['input_ids'],\n",
        "                                               train_features['attention_mask'],\n",
        "                                               train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIdt6DVlZbDm",
        "outputId": "35a89a38-608d-4f69-fc69-15142aecba8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Average Loss: 0.6373\n",
            "Epoch 2/3 - Average Loss: 0.3688\n",
            "Epoch 3/3 - Average Loss: 0.2468\n"
          ]
        }
      ],
      "source": [
        "# Fine-tuning loop\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtOKpalKcqoB",
        "outputId": "03db1e18-b4da-45a5-f1eb-d436f222a26a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fine_tuned_model/tokenizer_config.json',\n",
              " 'fine_tuned_model/special_tokens_map.json',\n",
              " 'fine_tuned_model/vocab.txt',\n",
              " 'fine_tuned_model/added_tokens.json',\n",
              " 'fine_tuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_model')\n",
        "tokenizer.save_pretrained('fine_tuned_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2CPjKgDAc1Pf"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "model_name = 'fine_tuned_model'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeFPeNjNc3TT",
        "outputId": "eef5645e-95d1-4833-9da7-2a4fc857a864"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name fine_tuned_model. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at fine_tuned_model were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Create SentenceTransformer for encoding\n",
        "sentence_transformer = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "c-189TQIc5Rr"
      },
      "outputs": [],
      "source": [
        "# Example inference\n",
        "queries = ['What is gradient descent?',]\n",
        "answers = ['The impact of climate change on ecosystems','Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function.',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1x3IZTi3c90-"
      },
      "outputs": [],
      "source": [
        "# Encode queries and answers into embeddings\n",
        "query_embeddings = sentence_transformer.encode(queries, convert_to_tensor=True)\n",
        "answer_embeddings = sentence_transformer.encode(answers, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TBfc_KkHc_tG"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between queries and answers\n",
        "cosine_scores = 1 - scipy.spatial.distance.cdist(query_embeddings.cpu(), answer_embeddings.cpu(), 'cosine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhB8JlTnaKLx",
        "outputId": "7e03515b-cec8-4bfe-9967-d1b6f821f1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is gradient descent?\n",
            "Top 2 Answers:\n",
            "Answer: The impact of climate change on ecosystems  Score: 0.4218\n",
            "Answer: Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function.  Score: 0.5071\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print results\n",
        "for i, query in enumerate(queries):\n",
        "    print(f'Query: {query}')\n",
        "    print('Top 2 Answers:')\n",
        "    for j in range(len(answers)):\n",
        "        answer = answers[j]\n",
        "        score = cosine_scores[i][j]\n",
        "        print(f'Answer: {answer}  Score: {score:.4f}')\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

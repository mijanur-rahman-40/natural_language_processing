{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine Tuning Sentence Tranformer\n",
        "- https://huggingface.co/blog/how-to-train-sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q7ReLxHVXmUe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install sentence-transformers\n",
        "#  sentence-transformers is a python framework for state-of-the-art sentence, text and image embeddings. It is backed by the popular HuggingFace transformers library. It provides a simple interface for computing embeddings while hiding the complex machinery behind it. It also supports fine-tuning of embeddings models on custom datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bVcerIvmXkLY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import InputExample, SentenceTransformer\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "v7HC3hCGX9cU"
      },
      "outputs": [],
      "source": [
        "# Prepare your training data\n",
        "train_examples = []\n",
        "\n",
        "# Generate train examples with keywords and labels\n",
        "train_examples = []\n",
        "\n",
        "# premise,hypothesis, label\n",
        "\n",
        "# Example 1\n",
        "sentence1 = \"The importance of renewable energy\"\n",
        "sentence2 = \"Renewable energy sources for a sustainable future\"\n",
        "label = 1  # Similar sentences\n",
        "\n",
        "train_examples.append(InputExample(texts=[sentence1, sentence2], label=label))\n",
        "\n",
        "# Example 2\n",
        "sentence1 = \"Applications of artificial intelligence in healthcare\"\n",
        "sentence2 = \"Artificial intelligence advancements in medical diagnosis\"\n",
        "label = 1  # Similar sentences\n",
        "\n",
        "train_examples.append(InputExample(texts=[sentence1, sentence2], label=label))\n",
        "\n",
        "# Example 3\n",
        "sentence1 = \"Climate change effects on biodiversity\"\n",
        "sentence2 = \"The impact of climate change on ecosystems\"\n",
        "label = 0  # Dissimilar sentences\n",
        "\n",
        "train_examples.append(InputExample(texts=[sentence1, sentence2], label=label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEVHotYbcZWF",
        "outputId": "6103ffe8-3772-4d33-e490-dba3f2e9cddb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/distilbert-base-nli-mean-tokens and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained Sentence Transformer model\n",
        "model_name = 'sentence-transformers/distilbert-base-nli-mean-tokens'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_hL6RG2AcbOO"
      },
      "outputs": [],
      "source": [
        "# Prepare your training data\n",
        "train_examples = train_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6I3WTlJycjy-"
      },
      "outputs": [],
      "source": [
        "# Tokenize and convert train examples to features\n",
        "train_features = tokenizer.batch_encode_plus(\n",
        "    [(example.texts[0], example.texts[1]) for example in train_examples],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "train_labels = torch.tensor([example.label for example in train_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "V48bJHe5clax"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning setup\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features['input_ids'],\n",
        "                                               train_features['attention_mask'],\n",
        "                                               train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIdt6DVlZbDm",
        "outputId": "35a89a38-608d-4f69-fc69-15142aecba8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Average Loss: 0.6316\n",
            "Epoch 2/3 - Average Loss: 0.5565\n",
            "Epoch 3/3 - Average Loss: 0.4972\n"
          ]
        }
      ],
      "source": [
        "# Fine-tuning loop\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtOKpalKcqoB",
        "outputId": "03db1e18-b4da-45a5-f1eb-d436f222a26a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fine_tuned_model/tokenizer_config.json',\n",
              " 'fine_tuned_model/special_tokens_map.json',\n",
              " 'fine_tuned_model/vocab.txt',\n",
              " 'fine_tuned_model/added_tokens.json',\n",
              " 'fine_tuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_model')\n",
        "tokenizer.save_pretrained('fine_tuned_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2CPjKgDAc1Pf"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "model_name = 'fine_tuned_model'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeFPeNjNc3TT",
        "outputId": "eef5645e-95d1-4833-9da7-2a4fc857a864"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name fine_tuned_model. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at fine_tuned_model were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Create SentenceTransformer for encoding\n",
        "sentence_transformer = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "c-189TQIc5Rr"
      },
      "outputs": [],
      "source": [
        "# Example inference\n",
        "queries = ['The importance of renewable energy',]\n",
        "answers = ['The impact of climate change on ecosystems','necessity of renewable energy',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1x3IZTi3c90-"
      },
      "outputs": [],
      "source": [
        "# Encode queries and answers into embeddings\n",
        "query_embeddings = sentence_transformer.encode(queries, convert_to_tensor=True)\n",
        "answer_embeddings = sentence_transformer.encode(answers, convert_to_tensor=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "TBfc_KkHc_tG"
      },
      "outputs": [],
      "source": [
        "# Calculate cosine similarity between queries and answers\n",
        "cosine_scores = 1 - scipy.spatial.distance.cdist(query_embeddings.cpu(), answer_embeddings.cpu(), 'cosine')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhB8JlTnaKLx",
        "outputId": "7e03515b-cec8-4bfe-9967-d1b6f821f1bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: The importance of renewable energy\n",
            "Top 2 Answers:\n",
            "Answer: The impact of climate change on ecosystems  Score: 0.5618\n",
            "Answer: necessity of renewable energy  Score: 0.9523\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print results\n",
        "for i, query in enumerate(queries):\n",
        "    print(f'Query: {query}')\n",
        "    print('Top 2 Answers:')\n",
        "    for j in range(len(answers)):\n",
        "        answer = answers[j]\n",
        "        score = cosine_scores[i][j]\n",
        "        print(f'Answer: {answer}  Score: {score:.4f}')\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

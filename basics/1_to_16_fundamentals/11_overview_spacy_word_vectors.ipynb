{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0a80da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# word vectors occupy lot of space. hence en_core_web_sm model do not have them included. \n",
    "# In order to download\n",
    "# word vectors you need to install large or medium english model. We will install the large one!\n",
    "# make sure you have run \"python -m spacy download en_core_web_lg\" to install large english model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# spacy word vectors means that each word is represented by a vector of 300 dimensions\n",
    "# this is a pretrained model. so we can use it directly\n",
    "# meaning of neumerical values is what the model has learned from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7b0ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog Vector: True OOV: False\n",
      "cat Vector: True OOV: False\n",
      "banana Vector: True OOV: False\n",
      "raju Vector: True OOV: False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"dog cat banana raju\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"Vector:\", token.has_vector, \"OOV:\", token.is_oov)\n",
    "    # token.has_vector - returns true if word vector is available for the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1213a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector\n",
    "doc[0].vector.shape\n",
    "\n",
    "# vector for the entire sentence\n",
    "# doc.vector\n",
    "\n",
    "# word vector meeans the vector representation of the word in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e62cde6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.1260e+00, -4.2692e+00, -7.9904e-01,  1.5811e+00,  2.6300e+00,\n",
       "       -5.6096e+00,  1.6773e+00,  5.7257e+00, -5.3666e+00,  6.0170e-01,\n",
       "        6.2670e+00,  1.0349e+00,  3.2992e+00,  1.7873e+00,  2.2344e+00,\n",
       "       -6.3331e+00,  9.8316e-01, -9.6958e-01,  1.0844e+00, -2.2912e+00,\n",
       "       -3.3732e+00,  2.5687e+00,  5.6621e-01, -2.7560e+00, -2.5407e+00,\n",
       "        1.3260e+00, -4.4180e+00,  3.8633e+00, -2.5972e+00,  1.2347e+00,\n",
       "        4.2847e+00, -2.4904e+00,  9.3384e-01, -1.5534e+00,  3.3076e-01,\n",
       "       -4.1011e+00, -1.2641e-02,  4.2600e+00, -9.5077e-01,  9.2123e-01,\n",
       "       -3.5137e-01,  7.0070e-01,  2.0004e+00,  5.8038e-01,  4.7097e+00,\n",
       "        4.5480e-01, -3.4912e-01,  2.1022e+00, -2.2968e+00, -4.0087e+00,\n",
       "       -3.1960e+00, -1.4585e+00,  5.1237e+00, -4.6956e+00, -4.1237e+00,\n",
       "       -1.3773e+00, -8.5791e-01,  2.5150e+00,  4.5625e+00,  2.0744e+00,\n",
       "        2.4309e+00, -1.5673e+00,  3.6562e+00, -7.8879e-01,  2.7344e+00,\n",
       "        9.6651e-01, -7.0019e+00, -2.3392e+00,  1.6044e+00,  1.3117e+00,\n",
       "       -1.2023e+00,  3.3839e+00, -2.9211e+00,  2.3648e+00, -3.8603e-01,\n",
       "        1.6597e+00, -4.6945e+00,  1.3763e+00,  2.1868e+00, -2.6112e+00,\n",
       "       -5.9549e+00, -1.2826e+00,  1.5707e+00,  2.2658e+00, -1.9748e+00,\n",
       "        3.5204e+00,  4.2247e+00, -3.0849e-01,  2.7008e+00, -3.0297e+00,\n",
       "        3.8421e+00,  3.0964e+00,  3.4938e+00, -6.3437e+00,  2.1439e+00,\n",
       "       -4.0967e+00,  5.1689e+00, -1.0605e+00, -1.3458e+00, -5.3143e+00,\n",
       "       -3.7146e-02, -6.2042e+00,  1.8335e-01, -5.1569e+00,  1.1898e+00,\n",
       "        2.9133e+00, -4.9812e+00,  1.9404e+00, -7.7228e-01, -3.7944e+00,\n",
       "        1.4160e+00, -1.1898e+00, -5.4947e-02,  8.3061e-01,  7.0460e+00,\n",
       "        5.1537e+00,  2.3402e+00, -3.7683e+00,  1.7732e+00,  5.3268e-01,\n",
       "       -1.3543e+00,  1.0313e-01, -1.9336e+00,  1.3415e+00,  5.9706e-01,\n",
       "       -4.6507e+00,  7.7987e-01, -7.2693e+00,  6.2300e+00, -1.0909e+00,\n",
       "       -2.8142e+00,  2.7780e+00,  4.4948e+00, -3.9099e-01, -7.0489e-01,\n",
       "        1.0479e+00, -3.2158e+00, -6.3542e+00,  9.3840e-02, -3.4020e+00,\n",
       "       -1.0503e+00, -1.0624e+00, -3.2737e+00, -2.0712e+00,  5.2742e-01,\n",
       "       -3.4039e-01, -2.5906e+00, -3.0968e-01,  2.8157e-01, -1.9718e+00,\n",
       "       -5.1772e-01,  1.4642e+00, -3.1157e+00,  4.0142e+00,  3.9149e-01,\n",
       "        4.3078e+00,  7.9816e+00,  2.3714e-01,  7.9726e-01, -6.6937e+00,\n",
       "       -3.0585e-01,  1.5462e-01, -2.0142e-01,  8.3269e-01,  6.3578e+00,\n",
       "       -2.5538e+00, -2.6023e+00,  1.1961e+00, -2.7659e-01,  1.0716e+00,\n",
       "       -5.5053e-01,  1.2944e+00,  7.1797e-01,  6.0816e+00, -9.9296e-01,\n",
       "       -2.9370e-02,  7.1808e-01, -1.3984e+00,  3.3786e+00, -1.6457e+00,\n",
       "        7.6552e-01,  3.5401e+00,  3.0240e+00,  1.6387e-01, -3.5823e+00,\n",
       "       -1.2986e+00, -1.3833e+00, -4.1255e+00,  2.5768e+00,  2.3428e+00,\n",
       "       -1.3700e+00,  3.2197e+00, -7.8306e-01,  4.3596e+00,  1.6687e+00,\n",
       "       -5.6499e+00, -3.0651e+00, -3.3438e-01,  2.2905e+00,  2.0919e+00,\n",
       "       -2.6097e+00, -1.5406e+00, -1.6686e+00, -9.1728e-01,  2.7626e+00,\n",
       "       -1.2651e+00, -7.8084e-01,  1.0267e+00,  3.7664e+00,  5.7199e+00,\n",
       "        3.0572e+00,  2.1356e+00, -5.7012e-01,  3.2591e-01,  1.4653e+00,\n",
       "       -3.9874e-01, -1.2697e+00,  5.8314e-01, -6.1736e-01, -7.0863e-01,\n",
       "        3.4501e+00,  3.6098e-01, -1.5018e+00,  4.3129e-02, -1.2609e+00,\n",
       "       -5.6140e+00, -1.0869e+00,  3.1231e+00,  5.2164e+00,  1.6839e+00,\n",
       "        2.8523e-01, -4.6565e+00,  6.5614e-01, -1.5697e+00, -2.3991e+00,\n",
       "        4.0071e+00,  3.8626e-01,  4.8020e+00,  4.7340e-01, -8.2624e-01,\n",
       "        1.0690e+00, -1.1616e+00,  1.4427e+00,  3.0298e+00,  1.1644e-01,\n",
       "        6.6460e-01,  5.2214e-03,  3.8662e+00,  8.6221e-01, -3.7953e+00,\n",
       "       -5.9257e+00,  7.5805e-01, -4.5844e+00, -1.6170e+00,  5.6344e-01,\n",
       "       -2.9060e+00, -1.9101e+00,  5.0515e+00, -2.9732e+00,  3.0460e+00,\n",
       "        4.3514e+00,  8.6091e+00, -2.9094e+00, -1.2126e+00, -1.5690e+00,\n",
       "       -9.2948e-01,  3.8603e+00,  3.2740e+00, -2.6038e+00,  3.7573e+00,\n",
       "        2.9347e+00,  9.5010e-01,  3.0991e-01, -3.8559e+00, -3.8878e+00,\n",
       "       -1.9464e+00,  2.3391e+00,  9.6548e-01,  1.1167e+00, -3.1489e-01,\n",
       "       -1.2282e-01, -4.1789e+00, -1.6211e+00, -2.7794e+00, -1.5756e+00,\n",
       "       -1.3622e+00, -2.4039e+00, -3.0955e+00, -1.0417e+00,  6.8707e-01,\n",
       "        4.3513e+00, -5.0665e-01, -3.4549e-01, -7.3201e+00,  4.9049e-01,\n",
       "       -5.1855e-02,  2.9423e+00,  2.2800e+00,  2.7997e+00, -1.1393e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_token = nlp(\"bread\")\n",
    "base_token.vector.shape\n",
    "\n",
    "# as a result of above command, we get a vector of 300 dimensions\n",
    "# as a individual word vector is of 300 dimensions\n",
    "# as an individual word vector size is equal to the size of the word vector of the entire sentence\n",
    "base_token.vector\n",
    "base_token[0].vector\n",
    "# this return array means that the word vector of the word \"bread\" is a 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "443e1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread <-> bread: 1.0\n",
      "sandwich <-> bread: 0.6341067010130894\n",
      "burger <-> bread: 0.47520687769584247\n",
      "car <-> bread: 0.06451533308853552\n",
      "tiger <-> bread: 0.04764611675903374\n",
      "human <-> bread: 0.2151154210812192\n",
      "wheat <-> bread: 0.6150360888607199\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"bread sandwich burger car tiger human wheat\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} <-> {base_token.text}:\", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c35619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(base_word, words_to_compare):\n",
    "    base_token = nlp(base_word)\n",
    "    doc = nlp(words_to_compare)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} <-> {base_token.text}: \", token.similarity(base_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4071a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple <-> iphone:  0.4387907401919904\n",
      "samsung <-> iphone:  0.670859081425417\n",
      "iphone <-> iphone:  1.0\n",
      "dog <-> iphone:  0.08211864228011527\n",
      "kitten <-> iphone:  0.10222317834969896\n"
     ]
    }
   ],
   "source": [
    "print_similarity(\"iphone\", \"apple samsung iphone dog kitten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daffd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "king = nlp.vocab[\"king\"].vector\n",
    "king\n",
    "man = nlp.vocab[\"man\"].vector\n",
    "woman = nlp.vocab[\"woman\"].vector\n",
    "queen = nlp.vocab[\"queen\"].vector\n",
    "\n",
    "result = king - man + woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab939b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61780137]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([result], [queen])\n",
    "\n",
    "# more than .5 is considered as good similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
